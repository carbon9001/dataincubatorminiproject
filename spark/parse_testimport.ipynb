{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "//for stacktable class\n",
    "import java.io.File\n",
    "import scala.io.{ BufferedSource, Source }\n",
    "\n",
    "abstract class StackTable[T] {\n",
    "\n",
    "  val file: File\n",
    "\n",
    "  def getDate(n: scala.xml.NodeSeq): Long = n.text match {\n",
    "    case \"\" => 0\n",
    "    case s => dateFormat.parse(s).getTime\n",
    "  }\n",
    "\n",
    "def getDateString(n: scala.xml.NodeSeq): Long = n.text match {\n",
    "    case \"\" => 0\n",
    "    case s => (s slice(11,13)).toInt\n",
    "  }\n",
    "\n",
    "  def dateFormat = {\n",
    "    import java.text.SimpleDateFormat\n",
    "    import java.util.TimeZone\n",
    "    val f = new SimpleDateFormat(\"yyyy-MM-dd'T'HH:mm:ss.SSS\")\n",
    "    f.setTimeZone(TimeZone.getTimeZone(\"GMT\"))\n",
    "    f\n",
    "  }\n",
    "\n",
    "  def getInt(n: scala.xml.NodeSeq): Int = n.text match {\n",
    "    case \"\" => 0 \n",
    "    case x => x.toInt\n",
    "  }\n",
    "def getAnswerId(n:scala.xml.NodeSeq):Int = n.text match{  \n",
    "    case \"\" => -5\n",
    "    case x => x.toInt  \n",
    "    }\n",
    "\n",
    "  def parseXml(x: scala.xml.Elem): T\n",
    "\n",
    "\n",
    "  def parse(s: String): Option[T] =\n",
    "    if (s.startsWith(\"  <row \") & s.endsWith(\" />\")) \n",
    "        Some(parseXml(scala.xml.XML.loadString(s)))\n",
    "    else None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "// for user \n",
    "import scala.xml.{ NodeSeq, MetaData }\n",
    "import java.io.File\n",
    "import scala.io.{ BufferedSource, Source }\n",
    "\n",
    "object User extends StackTable[User] {\n",
    "\n",
    "  val file = new File(\"\")\n",
    "  //val file = new File(\"data/allUsers/\")\n",
    "  //val file = new File(\"data/Users.xml\")\n",
    "//  assert(file.exists)\n",
    "\n",
    "  override def parseXml(x: scala.xml.Elem): User = User(\n",
    "    getInt(x \\ \"@Id\"),\n",
    "    getInt(x \\ \"@Reputation\"),\n",
    "    getDate(x \\ \"@CreationDate\"),\n",
    "    (x \\ \"@DisplayName\").text,\n",
    "    getDate(x \\ \"@LastAccessDate\"),\n",
    "    (x \\ \"@WebsiteUrl\").text,\n",
    "    (x \\ \"@Location\").text,\n",
    "    (x \\ \"@AboutMe\").text,\n",
    "    getInt(x \\ \"@Views\"),\n",
    "    getInt(x \\ \"@UpVotes\"),\n",
    "    getInt(x \\ \"@DownVotes\"),\n",
    "    (x \\ \"@EmailHash\").text,\n",
    "    getInt(x \\ \"@Age\"))\n",
    "}\n",
    "\n",
    "case class User(\n",
    "  id: Int,\n",
    "  reputation: Int,\n",
    "  creationDate: Long,\n",
    "  displayName: String,\n",
    "  lastAccessDate: Long,\n",
    "  websiteUrl: String,\n",
    "  location: String,\n",
    "  aboutMe: String,\n",
    "  views: Int,\n",
    "  upVotes: Int,\n",
    "  downVotes: Int,\n",
    "  emailHash: String,\n",
    "  age: Int) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "// for Posts\n",
    "\n",
    "object Post extends StackTable[Post] {\n",
    "\n",
    "    val file = new File(\"\")\n",
    "//  val file = new File(Dir+\"/allPosts/\")\n",
    " // val file = new File(\"data/Posts.xml\")\n",
    "//  assert(file.exists)\n",
    "\n",
    "  override def parseXml(x: scala.xml.Elem): Post = Post(\n",
    "    getInt(x \\ \"@Id\"),\n",
    "    getInt(x \\ \"@PostTypeId\"),\n",
    "    getInt(x \\ \"@ParentId\"),\n",
    "    getAnswerId(x \\ \"@AcceptedAnswerId\"),\n",
    "    getDate(x \\ \"@CreationDate\"),\n",
    "    getDateString(x \\ \"@CreationDate\"),\n",
    "    getInt(x \\ \"@Score\"),\n",
    "    getInt(x \\ \"@ViewCount\"),\n",
    "    (x \\ \"@Body\").text,\n",
    "    getInt(x \\ \"@OwnerUserId\"),\n",
    "    getDate(x \\ \"@LastActivityDate\"),\n",
    "    (x \\ \"@Title\").text,\n",
    "    getTags(x \\ \"@Tags\"),\n",
    "    getInt(x \\ \"@AnswerCount\"),\n",
    "    getInt(x \\ \"@CommentCount\"),\n",
    "    getInt(x \\ \"@FavoriteCount\"),\n",
    "    getDate(x \\ \"@CommunityOwnedDate\"))\n",
    "\n",
    "  def getTags(x: scala.xml.NodeSeq): Array[String] = x.text match {\n",
    "    case \"\" => Array()\n",
    "    case s => s.drop(1).dropRight(1).split(\"><\")\n",
    "  }\n",
    "}\n",
    "case class Post(\n",
    "  id: Int,\n",
    "  postTypeId: Int,\n",
    "  parentId: Int,\n",
    "  acceptedAnswerId: Int,\n",
    "  creationDate: Long,\n",
    "  creationhour:Long,\n",
    "  score: Int,\n",
    "  viewCount: Int,\n",
    "  body: String,\n",
    "  ownerUserId: Int,\n",
    "  lastActivityDate: Long,\n",
    "  title: String,\n",
    "  tags: Array[String],\n",
    "  answerCount: Int,\n",
    "  commentCount: Int,\n",
    "  favoriteCount: Int,\n",
    "  communityOwnedDate: Long) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "// for vote\n",
    "\n",
    "import scala.xml.{ NodeSeq, MetaData }\n",
    "import java.io.File\n",
    "import scala.io.{ BufferedSource, Source }\n",
    "\n",
    "object Vote extends StackTable[Vote] {\n",
    "\n",
    "  val file = new File(\"\")\n",
    " // val file = new File(\"data/allVotes/\")\n",
    "  //val file = new File(\"data/Votes.xml\")\n",
    "  //assert(file.exists)\n",
    "\n",
    "  override def parseXml(x: scala.xml.Elem): Vote = Vote(\n",
    "    getInt(x \\ \"@Id\"),\n",
    "    getInt(x \\ \"@PostId\"),\n",
    "    getInt(x \\ \"@VoteTypeId\"),\n",
    "    getInt(x \\ \"@UserId\"),\n",
    "    getDate(x \\ \"@CreationDate\"))\n",
    "}\n",
    "\n",
    "// <row Id=\"1264793\" PostId=\"486481\" VoteTypeId=\"5\" UserId=\"175880\" CreationDate=\"2013-05-30T00:00:00.000\" />\n",
    "case class Vote(\n",
    "  id: Int,\n",
    "  postId: Int,\n",
    "  voteTypeId: Int,\n",
    "  userId: Int,\n",
    "  creationDate: Long) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/plain": [
       "./data"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.SparkContext._\n",
    "import org.apache.spark.SparkConf\n",
    "//val inputDir = \"./test\"\n",
    "val inputDir = \"./data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "incomplete"
     ]
    }
   ],
   "source": [
    "/*\n",
    "val conf = new SparkConf();\n",
    "conf.setAppName(\"test\");\n",
    "conf.set(\"spark.driver.allowMultipleContexts\", \"true\");\n",
    "conf.setMaster(\"local[*]\");\n",
    "val sc = new SparkContext(conf)\n",
    "*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[2] at flatMap at <console>:33"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.rdd.RDD\n",
    "val testdata = spark.textFile(inputDir+\"/allPosts/\",minPartitions=2)\n",
    "val objData = testdata.flatMap(Post.parse)\n",
    "objData.cache\n",
    "var query: RDD[Post] = objData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[5] at flatMap at <console>:33"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "val jsonVoteData = spark.textFile(inputDir+\"/allVotes/\", minPartitions=2)\n",
    "val voteData = jsonVoteData.flatMap(Vote.parse)\n",
    "voteData.cache\n",
    "var queryVote: RDD[Vote] = voteData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[5] at flatMap at <console>:33"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val jsonUserData = spark.textFile(inputDir+\"/allUsers/\", minPartitions=2)\n",
    "val userData = jsonUserData.flatMap(User.parse)\n",
    "userData.cache\n",
    "var queryUser: RDD[User] = userData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "//q1\n",
    "val numUpvote = voteData.filter(_.voteTypeId==2).count()\n",
    "val numDownvote = voteData.filter(_.voteTypeId==3).count()\n",
    "println(\"Total upvotes:\"+numUpvote)\n",
    "println(\"Total downvotes:\"+numDownvote)\n",
    "        \n",
    "//***** count upVotes, downVotes and favVotes for each post\n",
    "// count UpVotes, DownVotes and FavVotes \n",
    "val upVotes = voteData.filter(_.voteTypeId==2).map(x=>(x.postId,1)).reduceByKey(_+_)\n",
    "val downVotes = voteData.filter(_.voteTypeId==3).map(x=>(x.postId,1)).reduceByKey(_+_)\n",
    "val favVotes = voteData.filter(_.voteTypeId==5).map(x=>(x.postId,1)).reduceByKey(_+_)\n",
    "        \n",
    "// join UpVotes, DownVotes and FavVotes counts, key by FavCounts\n",
    "val updownVotes = upVotes.fullOuterJoin(downVotes)\n",
    "                          .map(x=>(x._1,(x._2._1.getOrElse(0), x._2._2.getOrElse(0))))\n",
    "val updownfavVotes = updownVotes.fullOuterJoin(favVotes)\n",
    "                          .map(x=>(x._2._2.getOrElse(0), x._2._1.getOrElse((0,0))))\n",
    "        \n",
    "// group by Fav, calculate average ratio\n",
    "val favUpDown = updownfavVotes.reduceByKey((x,y)=>(x._1+y._1, x._2+y._2))\n",
    "                                      .mapValues(x=>x._1*1./(x._1+x._2))\n",
    "favUpDown.sortByKey().take(51).foreach(println)\n",
    "        \n",
    "//***** check points: Mean of top 50 favorite counts: 24.76\n",
    "println(\"Check point:\", updownfavVotes.countByKey.toSeq.sortBy(_._1).take(50).mkString(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start calculation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/plain": [
       "Array((0,(1604313,1550017094,2116239,550353,1404508)), (1,(586556,478630216,478263,225117,259556)))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//q6\n",
    "/*\n",
    "println(\"getting first ans post\")\n",
    "val firstAnsPosts = query.filter(_.postTypeId==1).keyBy(_.ownerUserId).groupByKey().mapValues(x => x.toSeq.sortBy(_.creationDate).take(1))\n",
    "println(firstAnsPosts.count())\n",
    "println(\"getting user create date\")\n",
    "val userCreaDate = queryUser.keyBy(_.id).mapValues(x=>x.creationDate)\n",
    "println(userCreaDate.count())\n",
    "println(\"getting veteran user\")\n",
    "val vetUser = query.keyBy(_.ownerUserId).join(userCreaDate).mapValues(x=>(x._1.creationDate - x._2)).filter(x => (x._2 >= 1000*3600*24*100L && x._2 <= 1000*3600*24*150L)).mapValues(x=>1).reduceByKey((x,y)=>x)\n",
    "println(vetUser.count())\n",
    "println(\"getting all result\")\n",
    "val statVetNew = firstAnsPosts.leftOuterJoin(vetUser).map(x=>(x._2._2.getOrElse(0),x._2._1(0))).mapValues(x =>(x.score, x.viewCount, x.answerCount, x.favoriteCount, 1)).reduceByKey((x,y)=>(x._1+y._1, x._2+y._2, x._3+y._3, x._4+y._4, x._5+y._5))\n",
    "println(statVetNew.count())\n",
    "println(\"print out\")\n",
    "\n",
    "statVetNew.foreach(println)\n",
    "*/\n",
    "\n",
    "//val keybyidpost =  query.filter(_.postTypeId==1).keyBy(_.ownerUserId)\n",
    "//keybyidpost.count()\n",
    "val userCreaDate = queryUser.keyBy(_.id).mapValues(x=>x.creationDate)\n",
    "//val vetUser = query.keyBy(_.ownerUserId).join(userCreaDate).mapValues(x=>(x._1.creationDate - x._2)).filter(x => (x._2 >= 1000*3600*24*100L && x._2 <= 1000*3600*24*150L)).mapValues(x=>1).reduceByKey((x,y)=>x)\n",
    "//userCreaDate.count()\n",
    "val vetUser = query.map(x=>(x.ownerUserId,x.creationDate)).join(userCreaDate).mapValues(x=>(x._1 - x._2)).filter(x => (x._2 >= 1000*3600*24*100L && x._2 <= 1000*3600*24*150L)).reduceByKey((_,_)=>{1}).map(x=>(x._1,1))\n",
    "val fisrtquestionpostsstruct = query.filter(_.postTypeId==1).map(x=>(x.ownerUserId,(x.creationDate,x.score, x.viewCount, x.answerCount, x.favoriteCount)))\n",
    "val firstquestionpost = fisrtquestionpostsstruct.reduceByKey((x,y)=>{if(x._1<y._1) x else y})\n",
    "val firstPostsVetNew = firstquestionpost.leftOuterJoin(vetUser).map(x=>(x._2._2.getOrElse(0),x._2._1))\n",
    "val statVetNew = firstPostsVetNew.mapValues(x=>(x._2,x._3,x._4,x._5,1)).reduceByKey((x,y)=>(x._1+y._1 , x._2+y._2 , x._3+y._3 , x._4+y._4 , x._5+y._5))\n",
    "println(\"start calculation\")\n",
    "statVetNew.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array((0,(44946,11887316,20742,12349,21316)), (1,(6442,1684192,2360,2365,1818)))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "statVetNew.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[114] at map at <console>:49"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//(userid,1)\n",
    "val vetUser = query.map(x=>(x.ownerUserId,x.creationDate)).join(userCreaDate).mapValues(x=>(x._1 - x._2)).filter(x => (x._2 >= 1000*3600*24*100L && x._2 <= 1000*3600*24*150L)).reduceByKey((_,_)=>{1}).map(x=>(x._1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/plain": [
       "Array((27115,1), (3454,1), (3443,1), (3586,1), (12518,1), (4499,1), (40073,1), (8657,1), (16137,1), (319,1), (28732,1), (6149,1), (49643,1), (48455,1), (32065,1), (58729,1), (25839,1), (17072,1), (6369,1), (1496,1), (32472,1), (58542,1), (52151,1), (18128,1), (10098,1), (40656,1), (35926,1), (48741,1), (7788,1), (27412,1))"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "println(vetUser.count())\n",
    "vetUser.take(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ShuffledRDD[117] at reduceByKey at <console>:43"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "//now we try to get the first question\n",
    "//|(userid,(creationdate,vet_views,vet_score,vet_favorites,vet_answers))\n",
    "val fisrtquestionpostsstruct = query.filter(_.postTypeId==1).map(x=>(x.ownerUserId,(x.creationDate,x.score, x.viewCount, x.answerCount, x.favoriteCount)))\n",
    "val firstquestionpost = fisrtquestionpostsstruct.reduceByKey((x,y)=>{if(x._1<y._1) x else y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/plain": [
       "23134"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "firstquestionpost.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[121] at map at <console>:54"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "val firstPostsVetNew = firstquestionpost.leftOuterJoin(vetUser).map(x=>(x._2._2.getOrElse(0),x._2._1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0,(1323656453027,0,468,0,0))\n",
      "23134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "println(firstPostsVetNew.first())\n",
    "println(firstPostsVetNew.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/plain": [
       "ShuffledRDD[123] at reduceByKey at <console>:56"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "val statVetNew = firstPostsVetNew.mapValues(x=>(x._2,x._3,x._4,x._5,1)).reduceByKey((x,y)=>(x._1+y._1 , x._2+y._2 , x._3+y._3 , x._4+y._4 , x._5+y._5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/plain": [
       "Array((0,(44946,11887316,20742,12349,21316)), (1,(6442,1684192,2360,2365,1818)))"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "println(statVetNew.count())\n",
    "statVetNew.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/plain": [
       "(44945,11887297,20742,12349,21316)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "val statpos = statVetNew.filter(x=>x._1==0).map(x=>(x._2._1,x._2._2,x._2._3,x._2._4,x._2._5)).reduce((x,y)=>(x._1+y._1 , x._2+y._2 , x._3+y._3 , x._4+y._4 , x._5+y._5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/plain": [
       "(3338,777656,1095,1376,762)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "val statvet = statVetNew.filter(x=>x._1==1).map(x=>(x._2._1,x._2._2,x._2._3,x._2._4,x._2._5)).reduce((x,y)=>(x._1+y._1 , x._2+y._2 , x._3+y._3 , x._4+y._4 , x._5+y._5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[66] at filter at <console>:58"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "val a = statVetNew.filter(x=>(x._1!=1&&x._1!=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/plain": [
       "(10267970170,(1,10,1,0,1))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": "Job aborted due to stage failure: Task 2 in stage 2.0 failed 1 times, most recent failure: Lost task 2.0 in stage 2.0 (TID 24, localhost): java.lang.ClassNotFoundException: $iwC$$iwC$$iwC$Post\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:381)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\tat java.lang.Class.forName0(Native Method)\n\tat java.lang.Class.forName(Class.java:348)\n\tat org.apache.spark.serializer.JavaDeserializationStream$$anon$1.resolveClass(JavaSerializer.scala:65)\n\tat java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1613)\n\tat java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1518)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1774)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2000)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1924)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:371)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:68)\n\tat org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)\n\tat org.apache.spark.Aggregator.combineCombinersByKey(Aggregator.scala:91)\n\tat org.apache.spark.shuffle.hash.HashShuffleReader.read(HashShuffleReader.scala:44)\n\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:92)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:244)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:64)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:197)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 2.0 failed 1 times, most recent failure: Lost task 2.0 in stage 2.0 (TID 24, localhost): java.lang.ClassNotFoundException: $iwC$$iwC$$iwC$Post\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:381)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\tat java.lang.Class.forName0(Native Method)\n\tat java.lang.Class.forName(Class.java:348)\n\tat org.apache.spark.serializer.JavaDeserializationStream$$anon$1.resolveClass(JavaSerializer.scala:65)\n\tat java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1613)\n\tat java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1518)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1774)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2000)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1924)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:371)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:68)\n\tat org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)\n\tat org.apache.spark.Aggregator.combineCombinersByKey(Aggregator.scala:91)\n\tat org.apache.spark.shuffle.hash.HashShuffleReader.read(HashShuffleReader.scala:44)\n\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:92)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:244)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:64)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:197)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:",
      "    org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1196)",
      "    org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1185)",
      "    org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1184)",
      "    scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)",
      "    scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)",
      "    org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1184)",
      "    org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:686)",
      "    org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:686)",
      "    scala.Option.foreach(Option.scala:236)",
      "    org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:686)",
      "    org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1377)",
      "    org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1338)",
      "    org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)"
     ]
    }
   ],
   "source": [
    "//val firstAnsPosts = keybyidpost.reduceByKey((x,y) => {if(x.creationDate <= y.creationDate) x else y})//.mapValues()\n",
    "val firstAnsPosts = keybyidpost.reduceByKey((x,y) =>x)\n",
    "firstAnsPosts.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('lattice', 0.9487745761871338),\n",
      "('shiny', 0.8836259245872498),\n",
      "('data.frame', 0.881213366985321),\n",
      "('r-grid', 0.8789350986480713),\n",
      "('lm', 0.8762001395225525),\n",
      "('plyr', 0.8754066228866577),\n",
      "('boxplot', 0.8743278980255127),\n",
      "('data.table', 0.8688276410102844),\n",
      "('na', 0.8672543168067932),\n",
      "('levelplot', 0.8644376993179321),\n",
      "('geom-bar', 0.8603816628456116),\n",
      "('quantmod', 0.8532227873802185),\n",
      "('reshape', 0.8528526425361633),\n",
      "('ggvis', 0.8518608212471008),\n",
      "('r-factor', 0.8515896201133728),\n",
      "('facet-wrap', 0.8468636274337769),\n",
      "('vegan', 0.8437350392341614),\n",
      "('dplyr', 0.8425593376159668),\n",
      "('lme4', 0.8422262668609619),\n",
      "('do.call', 0.8420583009719849),\n",
      "('plotmath', 0.840865969657898),\n",
      "('zoo', 0.8397353887557983),\n",
      "('rgl', 0.839381217956543),\n",
      "('gtable', 0.8386776447296143),\n",
      "('read.table', 0.8386289477348328),\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "//q8\n",
    "import org.apache.spark.mllib.linalg._\n",
    "//import org.apache.spark.mllib.feature.ElementwiseProduct\n",
    "import org.apache.spark.mllib.feature.{Word2Vec, Word2VecModel}\n",
    "\n",
    "       val q6_input = query.map(x=>x.tags.toSeq)\n",
    "\n",
    "       val word2vec = new Word2Vec()\n",
    "\n",
    "       val word2vec_model = word2vec.setSeed(42L).fit(q6_input)\n",
    "\n",
    "       val synonyms = word2vec_model.findSynonyms(\"ggplot2\", 25)\n",
    "\n",
    "       for((synonym, cosineSimilarity) <- synonyms) {\n",
    "         println(s\"('$synonym', $cosineSimilarity),\")\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "//q9\n",
    "//get questions\n",
    "//we try to get top 100 tags with largest count\n",
    "//try to get the records with tags from above\n",
    "//we try to tokenize all the record's body text\n",
    "//try to produce a vector from the body text\n",
    "//\n",
    "\n",
    "\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.sql.SQLContext\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[3] at filter at <console>:38"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "val question = query.filter(_.postTypeId==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/plain": [
       "52060"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/plain": [
       "ShuffledRDD[12] at reduceByKey at <console>:40"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "val tagcount = question.flatMap(x=>(x.tags)).map(x=>(x,1)).reduceByKey(_+_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/plain": [
       "Array(r, regression, time-series, machine-learning, probability, hypothesis-testing, distributions, self-study, logistic, correlation, classification, statistical-significance, bayesian, anova, normal-distribution, clustering, mathematical-statistics, confidence-interval, data-visualization, multiple-regression, estimation, categorical-data, mixed-model, generalized-linear-model, spss, variance, repeated-measures, sampling, t-test, pca, svm, forecasting, multivariate-analysis, chi-squared, cross-validation, maximum-likelihood, data-mining, modeling, neural-networks, data-transformation, predictive-models, matlab, nonparametric, interaction, survival, model-selection, p-value, linear-model, dataset, binomial, poisson, econometrics, standard-deviation, stata, mean, bootstrap, feature-selection, references, sample-size, interpretation, multiple-comparisons, optimization, least-squares, python, conditional-probability, random-forest, experiment-design, arima, prediction, panel-data, standaâ€¦"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val toptag = tagcount.sortBy(_._2,false).take(100).map(x=>(x._1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "ename": "java.lang.NoClassDefFoundError",
     "evalue": "Could not initialize class ",
     "output_type": "error",
     "traceback": [
      "java.lang.NoClassDefFoundError: Could not initialize class "
     ]
    }
   ],
   "source": [
    "//import org.apache.spark.SparkContext\n",
    "//import org.apache.spark.ml.classification.LogisticRegression\n",
    "//import org.apache.spark.ml.param.ParamMap\n",
    "//import org.apache.spark.mllib.linalg.{Vector, Vectors}\n",
    "//import org.apache.spark.mllib.regression.LabeledPoint\n",
    "//import org.apache.spark.sql.SQLContext\n",
    "\n",
    "//val sqlContext = new SQLContext(spark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "ename": "java.lang.NoClassDefFoundError",
     "evalue": "Could not initialize class ",
     "output_type": "error",
     "traceback": [
      "java.lang.NoClassDefFoundError: Could not initialize class "
     ]
    }
   ],
   "source": [
    "import sqlContext.implicits._\n",
    "\n",
    "val df = question.map(x=>(x.tags,x.body)).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/plain": [
       "./result4/"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val des = \"./result4/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "question.map(x => x.tags.mkString(\",\") + \",cyx0723,\" + x.id + \",cyx0723,\" + x.body + \",cyx0723recordend\").saveAsTextFile(des)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "def printToFile(f: java.io.File)(op: java.io.PrintWriter => Unit) {\n",
    "  val p = new java.io.PrintWriter(f)\n",
    "  try { op(p) } finally { p.close() }\n",
    "}\n",
    "import java.io._\n",
    "//val data = Array(\"Five\",\"strings\",\"in\",\"a\",\"file!\")\n",
    "printToFile(new File(\"toptag.txt\")) { p =>\n",
    "  toptag.foreach(p.println)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "val destr = \"./result5/train/\"\n",
    "val deste = \"./result5/test/\"\n",
    "\n",
    "val splited = question.map(x => x.tags.mkString(\",\") + \",cyx0723,\" + x.id + \",cyx0723,\" + x.body + \",cyx0723recordend\").randomSplit(Array(0.9,0.1),42)\n",
    "splited(0).saveAsTextFile(destr)\n",
    "splited(1).saveAsTextFile(deste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/plain": [
       "5212"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splited(1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/plain": [
       "Array(PartitionwiseSampledRDD[45] at randomSplit at <console>:43, PartitionwiseSampledRDD[46] at randomSplit at <console>:43)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "val testsp = question.randomSplit(Array(0.9,0.1),42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/plain": [
       "5212"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "testsp(1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "<console>:23: error: object SQLContext is not a member of package org.apache.spark.sql",
      "Note: class SQLContext exists, but it has no companion object.",
      "       import org.apache.spark.sql.SQLContext.implicits._",
      "                                   ^",
      "<console>:25: error: value toDF is not a member of org.apache.spark.rdd.RDD[Record]",
      "       val df = question.map(x=>Record(x.id,x.tags)).toDF()",
      "                                                     ^"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.SQLContext\n",
    "import org.apache.spark.sql.SQLContext.implicits._\n",
    "case class Record(id: Int, tags: Array[String])\n",
    "val df = question.map(x=>Record(x.id,x.tags)).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import org.apache.spark.sql.SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SQLContext@40d27d90"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class Record(id: Int, tags: Array[String])\n",
    "val sqlContext = new SQLContext(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "ename": "java.lang.NoSuchMethodError",
     "evalue": "org.apache.spark.sql.SQLContext$implicits$.intRddToDataFrameHolder(Lorg/apache/spark/rdd/RDD;)Lorg/apache/spark/sql/DataFrameHolder;",
     "output_type": "error",
     "traceback": [
      "java.lang.NoSuchMethodError: org.apache.spark.sql.SQLContext$implicits$.intRddToDataFrameHolder(Lorg/apache/spark/rdd/RDD;)Lorg/apache/spark/sql/DataFrameHolder;"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import sqlContext.implicits._\n",
    "val df = question.map(x=>x.id).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 1.3 (Scala 2.10)",
   "language": "scala",
   "name": "spark-1.3-scala-2.10"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": "scala",
   "mimetype": "text/x-scala",
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gzip, ujson, random, json\n",
    "import toolz\n",
    "import cPickle as c\n",
    "import nltk.tokenize\n",
    "import sklearn as sk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import linear_model\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def getjsontext(d):\n",
    "    return [toolz.keyfilter(lambda k: k == \"text\", d)]\n",
    "\n",
    "def load_json_yelp(filepath):\n",
    "    with gzip.open(\"yelp_train_academic_dataset_review.json.gz\") as f:\n",
    "        yelpdata = list()\n",
    "        for i in xrange(20000):\n",
    "            line = f.readline().strip()\n",
    "            data = json.loads(line)\n",
    "            yelpdata.append(data)\n",
    "    return yelpdata\n",
    "    \n",
    "def tokenizereview(str):\n",
    "    return nltk.tokenize.sent_tokenize(str)\n",
    "\n",
    "#data loading\n",
    "filepath = 'yelp_train_academic_dataset_review.json.gz'\n",
    "yelp_data = load_json_yelp(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "<type 'dict'>\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-5230fa55ed4a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myelp_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myelp_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mest_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0myelp_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mtt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_text\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mtt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "print len(yelp_data)\n",
    "print type(yelp_data)\n",
    "est_text = yelp_data[0]['text']\n",
    "tt=[test_text]\n",
    "print tt\n",
    "test_vec = CountVectorizer(encoding='unicode')\n",
    "a=test_vec.fit_transform(tt)\n",
    "\n",
    "#test_vec.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reviews = list()\n",
    "grades = list()\n",
    "for i in xrange(len(yelp_data)):\n",
    "    reviews.append(yelp_data[i]['text'])\n",
    "    grades.append(yelp_data[i]['stars'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=ShuffleSplit(20000, n_iter=20, test_size=0.2, random_state=42),\n",
       "       error_score='raise',\n",
       "       estimator=Pipeline(steps=[('counts', CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        ...it_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=None, solver='auto', tol=0.001))]),\n",
       "       fit_params={}, iid=True, n_jobs=2,\n",
       "       param_grid={'ridge__alpha': [1, 9], 'counts__min_df': [0.01, 0.05], 'counts__max_df': [0.99, 0.95]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='mean_squared_error',\n",
       "       verbose=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import cross_validation\n",
    "textcount_vectorizer = CountVectorizer()\n",
    "\n",
    "textcount_ridge = linear_model.Ridge(fit_intercept=True)\n",
    "\n",
    "counts_pipeline = Pipeline([\n",
    "       ('counts', textcount_vectorizer),\n",
    "       ('ridge', textcount_ridge)\n",
    "   ])\n",
    "\n",
    "params_grid = {\"counts__max_df\":[0.99,0.95], \"counts__min_df\":[0.01,0.05], \"ridge__alpha\":[1,9]}\n",
    "\n",
    "cv = cross_validation.ShuffleSplit(len(grades), n_iter=20, test_size=0.2, random_state=42)\n",
    "\n",
    "count_cross_validation = GridSearchCV(counts_pipeline, param_grid = params_grid, cv = cv, n_jobs = 2, scoring = 'mean_squared_error')\n",
    "\n",
    "count_cross_validation.fit(reviews, grades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ridge__alpha': 9, 'counts__min_df': 0.01, 'counts__max_df': 0.99}\n"
     ]
    }
   ],
   "source": [
    "print count_cross_validation.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import dill\n",
    "with open('test_bow','wb') as output:\n",
    "    dill.dump(count_cross_validation.best_estimator_,output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from data import test_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'>\n"
     ]
    }
   ],
   "source": [
    "print type(test_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'dict'>\n",
      "<type 'dict'>\n",
      "<type 'dict'>\n",
      "<type 'dict'>\n",
      "<type 'dict'>\n"
     ]
    }
   ],
   "source": [
    "testX=list()\n",
    "for j in test_json:\n",
    "    print type(j)\n",
    "    testX.append(j['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Love it!!!!! Love it!!!!!! love it!!!!!!!   Who doesn't love Culver's!\", 'Everything was great except for the burgers they are greasy and very charred compared to other stores.', 'I really like both Chinese restaurants in town.  This one has outstanding crab rangoon.  Love the chicken with snow peas and mushrooms and General Tso Chicken.  Food is always ready in 10 minutes which is accurate.  Good place and they give you free pop.', 'Above average takeout with friendly staff. The sauce on the pan fried noodle is tasty. Dumplings are quite good.', \"We order from Chang Jiang often and have never been disappointed.  The menu is huge, and can accomodate anyone's taste buds.  The service is quick, usually ready in 10 minutes.\"]\n"
     ]
    }
   ],
   "source": [
    "print testX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_pipe =count_cross_validation.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.22328161])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_pipe.predict([testX[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
